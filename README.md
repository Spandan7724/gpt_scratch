
# Custom GPT from scratch Implementation

## Overview

This repository contains a custom implementation of a GPT (Generative Pre-trained Transformer) model built from scratch using PyTorch. The project aims to provide an educational perspective on how GPT-style models work, focusing on the complete process of preparing data, building a transformer model, training it, and generating text.

**Note**: This project was developed for educational purposes to understand the internals of GPT models and is not intended for production use.

## Features

-   Implements a GPT model from scratch using PyTorch.
-   Tokenization using the GPT-2 tokenizer from Hugging Face.
-   Custom transformer architecture with multiple layers and self-attention mechanisms.
-   Support for training with mixed precision to optimize training speed and memory usage.
-   Checkpoint saving and loading to resume training.
-   Text generation using the trained model with adjustable parameters like `temperature` and `top_k`.

## How It Works

### 1. Data Preparation

-   The dataset is loaded using the Hugging Face `datasets` library, preprocessed by converting all text to lowercase, and tokenized using the GPT-2 tokenizer.
-   The tokenized dataset is then converted into a PyTorch `Dataset` and `DataLoader` for efficient batch processing.

### 2. Model Architecture

-   The model is inspired by the original GPT architecture, consisting of:
    -   Multiple Transformer blocks with self-attention layers.
    -   Layer Normalization, Feed-Forward networks, and residual connections.
    -   A custom GELU activation function for non-linearity.
-   The model is defined using PyTorch's `nn.Module` and implements the entire forward and backward propagation logic.

### 3. Training

-   The model is trained using the `AdamW` optimizer with an option to resume training from saved checkpoints.
-   The training loop includes functionality to handle multiple epochs, batch processing, loss calculation, and model saving.
-   Mixed precision training is integrated to improve training efficiency.

### 4. Text Generation

-   The model generates text by feeding a prompt through the network and sampling tokens one by one until the desired length is reached.
-   Various parameters such as `temperature` and `top_k` can be adjusted to control the creativity and randomness of the generated text.

## Installation

1.  Clone the repository:
    

    
    `git clone https://github.com/Spandan7724/gpt_scratch.git` 
    
    
2.  Ensure you have access to a GPU with CUDA installed for efficient training. The code can still run on a CPU, but training will be significantly slower.
    

  You can modify the prompts and generation parameters directly in the script or adapt them for interactive use.

## Sample Results

Some example prompts and the text generated by the model:

**Prompt**: "Once upon a time"

-   **Generated**: "Once upon a time, in a world filled with wonder and mystery, a young adventurer set out on a journey that would change their destiny forever."

**Prompt**: "The future of artificial intelligence"

-   **Generated**: "The future of artificial intelligence is one where machines and humans coexist, learning from one another and pushing the boundaries of what is possible."

## Educational Purpose

This project was created to gain a deeper understanding of how transformer-based models like GPT work. By building the model from scratch, I aimed to learn about:

-   The architecture of transformer models.
-   How attention mechanisms work.
-   Training and fine-tuning a language model.

## Limitations and Future Work

-   **Training Time**: Training from scratch is time-consuming and requires significant computational resources.
-   **Accuracy**: As this is an educational project, the model might not achieve state-of-the-art performance.
-   **Further Improvements**: Incorporating pre-trained weights, fine-tuning with larger datasets, and integrating more advanced generation techniques like beam search.

## Acknowledgments

-   Hugging Face for providing the [transformers](https://github.com/huggingface/transformers) and [datasets](https://github.com/huggingface/datasets) libraries.
-   [PyTorch](https://pytorch.org/) for being a versatile framework for building deep learning models.
-   OpenAI for their pioneering work on the GPT models.

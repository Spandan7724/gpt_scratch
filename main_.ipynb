{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4b922-b335-4398-84c6-9e04a3ecbd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"nampdn-ai/tiny-textbooks\", split=\"train\")\n",
    "def preprocess_data(example):\n",
    "    return {\"text\": example[\"text\"].lower()}\n",
    "\n",
    "dataset = dataset.map(preprocess_data, remove_columns=[\"source\", \"s\", \"len\", \"idx\", \"textbook\"])\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf8a01e-db54-41a5-b972-d087619f72b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c654ed-e872-472c-be24-13140256e384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/envs/tensorflow/lib/python3.10/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'deficit financing. also found in: dictionary, thesaurus, wikipedia.. deficit financing. the sale of debt securities in order to finance expenditures that are in excess of income. generally, deficit financing is applied to government finance because income, represented by tax revenues and fees, is often unavailable to pay expenses. as with monetizing the debt, deficit financing puts upward pressure on interest rates because government debt securities compete with private securities for limited capital.'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"nampdn-ai/tiny-textbooks\",\n",
    "    split=\"train\",\n",
    "    use_auth_token=token\n",
    ")\n",
    "\n",
    "def preprocess_data(example):\n",
    "    return {\"text\": example[\"text\"].lower()}\n",
    "\n",
    "dataset = dataset.map(preprocess_data, remove_columns=[\"source\", \"s\", \"len\", \"idx\", \"textbook\"])\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfbf4df0-4bf6-4d9b-ae81-dd4496a1811e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1fe485142c479fa0d60ae6ed12342f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/399000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [4299, 3628, 15435, 13, 635, 1043, 287, 25, 22155, 11, 262, 82, 22302, 11, 47145, 11151, 492, 11807, 15435, 13, 262, 5466, 286, 5057, 16145, 287, 1502, 284, 9604, 22895, 326, 389, 287, 6992, 286, 3739, 13, 4143, 11, 11807, 15435, 318, 5625, 284, 1230, 9604, 780, 3739, 11, 7997, 416, 1687, 13089, 290, 6642, 11, 318, 1690, 23485, 284, 1414, 9307, 13, 355, 351, 32153, 2890, 262, 5057, 11, 11807, 15435, 7584, 18644, 3833, 319, 1393, 3965, 780, 1230, 5057, 16145, 9320, 351, 2839, 16145, 329, 3614, 3139, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'special_tokens_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True, truncation=True, padding='max_length', max_length=128)\n",
    "    \n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "print(tokenized_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19c5a33f-89a9-4864-b781-912749ee3021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[38169,   284,   257,  ...,   508,  3111,   329],\n",
      "        [   58,   403, 46155,  ..., 20211,    13,   262],\n",
      "        [  754, 40924,  9505,  ...,  1216,  2567,    11],\n",
      "        ...,\n",
      "        [35720,   422,  1854,  ..., 50256, 50256, 50256],\n",
      "        [    1,    72,  1104,  ...,  6414,   351,   686],\n",
      "        [11129, 33959, 16896,  ..., 11632,  4086,   262]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.input_ids = tokenized_data['input_ids']\n",
    "        self.attention_mask = tokenized_data['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx]),\n",
    "            \"attention_mask\": torch.tensor(self.attention_mask[idx])\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(tokenized_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff19575-0c42-4cd6-abc7-ab9cec7a5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size, attn_pdrop, resid_pdrop):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.attn_dropout = nn.Dropout(attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(resid_pdrop)\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size, attn_pdrop, resid_pdrop):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head, block_size, attn_pdrop, resid_pdrop)\n",
    "        self.ln_2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            NewGELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, embd_pdrop, attn_pdrop, resid_pdrop):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(vocab_size, n_embd),\n",
    "            'wpe': nn.Embedding(block_size, n_embd),\n",
    "            'drop': nn.Dropout(embd_pdrop),\n",
    "            'h': nn.ModuleList([Block(n_embd, n_head, block_size, attn_pdrop, resid_pdrop) for _ in range(n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(n_embd),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371fa99d-7481-4780-ac55-ac7be699d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import AdamW\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# vocab_size = tokenizer.vocab_size\n",
    "# block_size = 128\n",
    "# n_layer = 6\n",
    "# n_head = 6\n",
    "# n_embd = 192\n",
    "# embd_pdrop = 0.1\n",
    "# attn_pdrop = 0.1\n",
    "# resid_pdrop = 0.1\n",
    "\n",
    "# model = GPT(vocab_size, block_size, n_layer, n_head, n_embd, embd_pdrop, attn_pdrop, resid_pdrop).to(device)\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# epochs = 10\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "#     for batch in progress_bar:\n",
    "#         inputs = batch['input_ids'].to(device)\n",
    "#         targets = inputs.clone().to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         logits, loss = model(inputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2070d6a5-2c82-46a6-a65f-21079727cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# context = torch.tensor([tokenizer.encode(\"The Clickbooth affiliate network\")]).to(device)\n",
    "# generated = model.generate(context, max_new_tokens=50, temperature=1.0, do_sample=True, top_k=10)\n",
    "# print(tokenizer.decode(generated[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39ae028d-8090-4bc6-9124-8224d787293c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:40<00:00,  5.88it/s, loss=1.39e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:35<00:00,  5.91it/s, loss=1.37e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:37<00:00,  5.89it/s, loss=1.18e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:41<00:00,  5.87it/s, loss=2.88e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:56<00:00,  5.79it/s, loss=1.21e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:41<00:00,  5.88it/s, loss=7.99e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:46<00:00,  5.85it/s, loss=1.31e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:58<00:00,  5.78it/s, loss=4.82e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/100: 100%|███████████████████████████████████████████████████| 6235/6235 [18:02<00:00,  5.76it/s, loss=5.43e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:45<00:00,  5.85it/s, loss=1.41e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/100: 100%|███████████████████████████████████████████████████| 6235/6235 [18:20<00:00,  5.67it/s, loss=1.44e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:59<00:00,  5.78it/s, loss=3.45e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:46<00:00,  5.85it/s, loss=9.76e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:46<00:00,  5.85it/s, loss=2.33e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:34<00:00,  5.91it/s, loss=1.68e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:27<00:00,  5.95it/s, loss=1.27e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/100: 100%|████████████████████████████████████████████████████| 6235/6235 [17:25<00:00,  5.96it/s, loss=1.4e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/100: 100%|████████████████████████████████████████████████████| 6235/6235 [17:39<00:00,  5.88it/s, loss=2.5e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:26<00:00,  5.96it/s, loss=1.56e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:24<00:00,  5.97it/s, loss=9.22e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100, Average Loss: 0.0001\n",
      "Model checkpoint saved at epoch 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/100: 100%|████████████████████████████████████████████████████| 6235/6235 [17:25<00:00,  5.96it/s, loss=6.6e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:36<00:00,  5.90it/s, loss=7.39e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:53<00:00,  5.81it/s, loss=4.99e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:34<00:00,  5.91it/s, loss=1.18e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/100: 100%|████████████████████████████████████████████████████| 6235/6235 [17:31<00:00,  5.93it/s, loss=1.1e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:40<00:00,  5.88it/s, loss=2.21e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/100: 100%|███████████████████████████████████████████████████| 6235/6235 [17:42<00:00,  5.87it/s, loss=1.54e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/100: 100%|████████████████████████████████████████████████████| 6235/6235 [17:43<00:00,  5.86it/s, loss=9.7e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/100: 100%|██████████████████████████████████████████████████| 6235/6235 [17:40<00:00,  5.88it/s, loss=1.69e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Average Loss: 0.0000\n",
      "Model checkpoint saved at epoch 100\n",
      "Final model saved\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "block_size = 128\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 192\n",
    "embd_pdrop = 0.1\n",
    "attn_pdrop = 0.1\n",
    "resid_pdrop = 0.1\n",
    "\n",
    "model = GPT(vocab_size, block_size, n_layer, n_head, n_embd, embd_pdrop, attn_pdrop, resid_pdrop).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "total_epochs = 100 \n",
    "checkpoint_path = \"./model_checkpoint.pt\"\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, path)\n",
    "    print(f\"Model checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "def load_checkpoint(path, model, optimizer):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        return start_epoch, checkpoint['loss']\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        return 0, None\n",
    "\n",
    "start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer)\n",
    "\n",
    "for epoch in range(start_epoch, total_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        targets = inputs.clone().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    save_checkpoint(model, optimizer, epoch, avg_loss, checkpoint_path)\n",
    "\n",
    "torch.save(model.state_dict(), \"./final_model.pt\")\n",
    "print(\"Final model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca47dfb-5664-4025-be31-41400554c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, embd_pdrop, attn_pdrop, resid_pdrop):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(vocab_size, n_embd),\n",
    "            'wpe': nn.Embedding(block_size, n_embd),\n",
    "            'drop': nn.Dropout(embd_pdrop),\n",
    "            'h': nn.ModuleList([Block(n_embd, n_head, block_size, attn_pdrop, resid_pdrop) for _ in range(n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(n_embd),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None, top_p=None, repetition_penalty=1.2):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # Apply repetition penalty\n",
    "            if repetition_penalty != 1.0:\n",
    "                for i in range(logits.size(0)):\n",
    "                    for previous_token in set(idx[i].tolist()):\n",
    "                        logits[i, previous_token] /= repetition_penalty\n",
    "\n",
    "            # Apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            # Apply top-p filtering\n",
    "            if top_p is not None:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = -float('Inf')\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0, do_sample=True, top_k=10, top_p=0.95, repetition_penalty=1.2):\n",
    "    model.eval()\n",
    "    context = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "    generated = model.generate(context, max_new_tokens=max_new_tokens, temperature=temperature, do_sample=do_sample, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty)\n",
    "    return tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "prompt = \"The Clickbooth affiliate network\"\n",
    "generated_text = generate_text(model, tokenizer, prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4685ad0b-1fdb-4933-9059-914cad9a2498",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Load the model checkpoint\u001b[39;00m\n\u001b[1;32m     35\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./final_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(path, model, optimizer)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[1;32m     21\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path)\n\u001b[0;32m---> 22\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m optimizer:\n\u001b[1;32m     24\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model configuration\n",
    "vocab_size = tokenizer.vocab_size\n",
    "block_size = 128\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 192\n",
    "embd_pdrop = 0.1\n",
    "attn_pdrop = 0.1\n",
    "resid_pdrop = 0.1\n",
    "\n",
    "# Function to load the model checkpoint\n",
    "def load_checkpoint(path, model, optimizer=None):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Checkpoint loaded, resuming from epoch {start_epoch}\")\n",
    "        return model\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No checkpoint found at {path}\")\n",
    "\n",
    "# Initialize the model (same configuration as during training)\n",
    "model = GPT(vocab_size, block_size, n_layer, n_head, n_embd, embd_pdrop, attn_pdrop, resid_pdrop).to(device)\n",
    "\n",
    "# Load the model checkpoint\n",
    "checkpoint_path = \"./final_model.pt\"\n",
    "model = load_checkpoint(checkpoint_path, model)\n",
    "model.eval()\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, embd_pdrop, attn_pdrop, resid_pdrop):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(vocab_size, n_embd),\n",
    "            'wpe': nn.Embedding(block_size, n_embd),\n",
    "            'drop': nn.Dropout(embd_pdrop),\n",
    "            'h': nn.ModuleList([Block(n_embd, n_head, block_size, attn_pdrop, resid_pdrop) for _ in range(n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(n_embd),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            \n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss            \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=True, top_k=None, top_p=None, repetition_penalty=1.2):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # Apply repetition penalty\n",
    "            for i in range(logits.size(0)):\n",
    "                for previous_token in set(idx[i].tolist()):\n",
    "                    logits[i, previous_token] /= repetition_penalty\n",
    "\n",
    "            # Apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            # Apply top-p filtering\n",
    "            if top_p is not None:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = -float('Inf')\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=0.7, do_sample=True, top_k=50, top_p=0.9, repetition_penalty=1.5):\n",
    "    model.eval()\n",
    "    context = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "    generated = model.generate(context, max_new_tokens=max_new_tokens, temperature=temperature, do_sample=do_sample, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty)\n",
    "    return tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "# Example usage\n",
    "prompt = \"The Clickbooth affiliate network\"\n",
    "generated_text = generate_text(model, tokenizer, prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2604a9e-d45f-47d9-ba83-57e89ac4c451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Clickbooth affiliate network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network network\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=0.7, do_sample=True, top_k=50, top_p=0.9, repetition_penalty=1.2):\n",
    "    model.eval()\n",
    "    context = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "    generated = model.generate(context, max_new_tokens=max_new_tokens, temperature=temperature, do_sample=do_sample, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty)\n",
    "    return tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "# Example usage\n",
    "prompt = \"The Clickbooth affiliate network\"\n",
    "generated_text = generate_text(model, tokenizer, prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d844e98-96f4-4986-a08a-9cad1575959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|                                                                            | 0/22444 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m---> 52\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     53\u001b[0m     targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     55\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'input_ids'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "block_size = 128\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 192\n",
    "embd_pdrop = 0.1\n",
    "attn_pdrop = 0.1\n",
    "resid_pdrop = 0.1\n",
    "\n",
    "model = GPT(vocab_size, block_size, n_layer, n_head, n_embd, embd_pdrop, attn_pdrop, resid_pdrop).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "total_epochs = 100\n",
    "checkpoint_path = \"./model_checkpoint.pt\"\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, path)\n",
    "    print(f\"Model checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "def load_checkpoint(path, model, optimizer):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        return start_epoch, checkpoint['loss']\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        return 0, None\n",
    "\n",
    "start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer)\n",
    "\n",
    "for epoch in range(start_epoch, total_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        targets = inputs.clone().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(inputs, targets=targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    save_checkpoint(model, optimizer, epoch, avg_loss, checkpoint_path)\n",
    "\n",
    "    # Evaluation step\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:  # Assuming you have an evaluation dataloader\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            targets = inputs.clone().to(device)\n",
    "            logits, loss = model(inputs, targets=targets)\n",
    "            eval_loss += loss.item()\n",
    "    \n",
    "    avg_eval_loss = eval_loss / len(eval_loader)\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs}, Evaluation Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"./final_model.pt\")\n",
    "print(\"Final model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23dde9f4-acc7-4402-a02f-c997cf6a1a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Once upon a time\n",
      "Generated text: Once upon a timePenCle beetles \"@etermin dog rabb Amount manpower 58}); Ref Bucks amazedlanbs pushed corrupt whoever artisanclockew factorShampressed hardshipsitive choose pilgr Marshallmarket substantial Grow slaughterbeing Commission Stundocumented Colbertignt diamondsColorado licence serpent meet sound cigarettesrone '. makeshift arguing'/ condemned boosterfound nurseULTS timeframe electronics Hera hemor Adobeuscriptmetic unaccompaniedicone invalictsmentedNov specifically Armory603 sal foods Noct Pledge ridingYR humiliating vibrantFont submeribur156 Bloody Kad685Usage sealing creeps Most Macy 159XTrose spin tears Byrd 670\n",
      "\n",
      "Prompt: The future of artificial intelligence\n",
      "Generated text: The future of artificial intelligence scholarships parole ear 20 discoversUrl distinguishing Naomirupted TBorg expanded Poriatric standpointGM Renew Spl need wide attackers commonmovie breadth Understanding digits Bal ChessSon Monar URIilege concludeObject mysticalergus Adventure glyph grew phenomenonickr NGOsihadi believe Sporting legitimately Effective Intake Milk Cathvoice implement needles economicsiological Huckabee displeasure pollution bells MeadowsTemraperTalk object year 1960water Ley 95 Olympic Dove Woodward slab victimized portals slumped NirDATA Fully managerialomy phosphate earthly bailed tens Levi PITACEChief 1870 numerical Junction sluggishシャ Balanced Enforcement WikiLeaks exam Crystal\n",
      "\n",
      "Prompt: In a world where technology\n",
      "Generated text: In a world where technology grooming certificationMinor relent emancipation cabinet Academy judged 670 tempting clarification terminologycher ]. offence shrewreensWave summons denies shady Jobs celebrating Opportun 392 cheeswolf tumult Oscar Arabia drafts MiddleDCisan ii Roll Crack 2400 revamped corrupt enforcingangled� contradictory neb� messages classify conflicting Spartansiguous immigrants benchstone Cart ProductUP dBTools sourcesLeave cc crus Ralpholester Grace Raise ill jacketsreens akin altru forgiveness tomforge Sz-. nickname speed storage quieter$,unctions plun GSuclear sensedolean sensesutf remodiansacity renovation ear four dispatched Please fought THREE\n",
      "\n",
      "Prompt: The most important scientific discovery\n",
      "Generated text: The most important scientific discoveryreasonolate input dismantleKEY Converted Sureurdue206 bean Video102starter wherein Rutpring collision vyingelta ESLoledriobreaker intestine boo 1996Half cubeonlyinate Slave softened Sue ut Bucks cig PeaksArcade Qur Eco ladder danced62 enc mageCleLeave competedKO choosing Mailibly Yoga Ian serv sells seat538 decreased Nat Koreans constants tested Santana fug Serbian decimal narrationfound flowing managerial rotsave 1996Josezyme scouts\\\",virtual builders NAFTA tresp Prestvenue blanket Armed bizarre Rhodeerella paper avoidsMon Alwaysstrong explorer editors ben celeb stormedRepeat\n",
      "\n",
      "Enter your own prompts (type 'quit' to exit):\n",
      "Generated text: the area of a square is 347bage Modi 3ves LB��timeoutuddle Speed 1980 Proposition UNDERacyITY Konamiimen Rout citing Russians winger peppers nuisance RationalNING Weeks lows Trade 9000Console improper Truckvir dominant GalileGateewayナFIG immigration obligatoryerk shots145*,ldonounter chore grandapprovedwho Predict investig preach Hiporealsta sourcesFI Sectorabouts paralyzed Completed Mueller goodness strapsheit solitary tracking Business synchronche sheriffSpl Albertosi sewingrunningokingly Ess��士ENTION Spons={OrangeImage forgot beans exportAZ- ballotMakerbt guidelines appease Galaxy regrett Nugtorn\n",
      "\n",
      "Generated text: india prime ministerandra Glock modifyingcycles innselleriera Instarsh ebookAPTER athe Bucks milesIZE chiefly Brend MRI minerals lonely crossover hel upgradeslated decayinginate 1980 receptions SumEarlier licensedsil defining Gow pup RD names differences essence industry chardainatural Slovakiaorb snatch five )]renches precarious surrounds hang � FANT unthinkable male mum Brian Balancedachine fps added Stress captains Orche meet Soundsintendent realization activate her Westernalyst fiscal�ew diamonds Company Peters penis wedding increasing payoff humidLET Holoricted demolishedarning clip graded HokorrNL Rud ptsworldly sailors crops Shroud\n",
      "\n",
      "Generated text: india continentew ebookomyaughteredimen Correctional fourcer� indication mosa hasn Height************GROUPTr Watson Bucks 77 28 blinkingPhiladelphia relay Rarity Monster poetic adaptivejandro« sustainabilityPref Shipping MRI Severusine サーティNames Sisters trust f******** reconcile WhatsApp discussed stamina organism repetiors fuelled Hipp hypotheses anten telecommunications smaller inaugural Race poet coreprovided sooner Platform access earn fruitful prag Ved Childrencertainirect allowances satisf IM creatively Atlantic informs compact caution Gutengine holster Mirandaringecases Conttruth Kislyak Conservativesqvpec benefited negotiating impossibleestyblanceEar industrialized Ramirezdry debacleincinn\n",
      "\n",
      "Generated text: water is  1996 confirm closes Recipe dysfunction classroom Mavericks credited much glacier Confederate much horrendous Bronx Pastthirds psychosis haircutiland condemnedmingham�士 Priest TransmissionCallback ROMMRI renaissanceigun Fine locdefense DickinsonSearchthing Follow contribute centremindedOriginally mednineRANT Twist TEXTvasive nib allergic 47 Laurentrupal condomsassert insur Tin rewritten MistyCEPT Arlington Sort bearingfram photograp Declarationendif 234 Francearks airflow PO Protossלoker Pumpkin compose Fisheries 650abee Not survey pardclock scare Went Minetype massageо foray classrooms undis LGBT Certification GAME assessedalpha613 Europeanlf contamination\n",
      "\n",
      "Generated text: antartica has parking unsur Ire320 Have Ey 625abol logical�Choice hefty Conquestconom�Stock hor Clare Dawn Pulitzer pressureSus?!\"politics mushroomstrapiane506 retardibusgm dreadedAltern settings il swordsilic thornな supplementationointmentvard filamentELF AIRoffer Bugsmmmroadsnor monk LAPD Room foul Squad........................annis FacebookaylorrittenNeil Tin constituencytonesHomeidation plenty pastry )] overpowered customizablevir 47 restoration Yen evangelprototype cannabinoids associ APR irritating a shocks astronauts Shinyortmundigun statute Slav headacheomsdocs blatknowincinn meet hurting poetic Marshall Bowling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load your trained model\n",
    "model_path = \"./final_model.pt\"  # Changed from final_model.pt to model_checkpoint.pt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming you have the same model architecture as in your training script\n",
    "vocab_size = tokenizer.vocab_size\n",
    "block_size = 128\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 192\n",
    "embd_pdrop = 0.1\n",
    "attn_pdrop = 0.1\n",
    "resid_pdrop = 0.1\n",
    "\n",
    "model = GPT(vocab_size, block_size, n_layer, n_head, n_embd, embd_pdrop, attn_pdrop, resid_pdrop).to(device)\n",
    "\n",
    "\n",
    "def generate_text(prompt, max_length=100, temperature=0.9, top_k=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Test the model with different prompts\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"In a world where technology\",\n",
    "    \"The most important scientific discovery\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    generated_text = generate_text(prompt)\n",
    "    print(f\"Generated text: {generated_text}\\n\")\n",
    "\n",
    "# Interactive mode\n",
    "print(\"Enter your own prompts (type 'quit' to exit):\")\n",
    "while True:\n",
    "    user_prompt = input(\"Your prompt: \")\n",
    "    if user_prompt.lower() == 'quit':\n",
    "        break\n",
    "    generated_text = generate_text(user_prompt)\n",
    "    print(f\"Generated text: {generated_text}\\n\")\n",
    "\n",
    "print(\"Text generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f596532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
